{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini API with Kotlin\n",
    "\n",
    "This notebook demonstrates how to use the Google Gemini API with Kotlin through the Google GenAI Java SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Google Gemini\n",
    "\n",
    "Google Gemini is a family of multimodal large language models (LLMs) developed by Google DeepMind. These models are designed to understand and generate content across different modalities including text, images, audio, video, and code. Gemini models excel at complex reasoning tasks, creative content generation, and can process multiple types of information simultaneously.\n",
    "\n",
    "Key features of Gemini include:\n",
    "\n",
    "- **Multimodal capabilities**: Can understand and generate content across different formats\n",
    "- **Advanced reasoning**: Capable of solving complex problems and following nuanced instructions\n",
    "- **Code generation and understanding**: Strong performance on programming tasks\n",
    "- **Real-time interaction**: Support for streaming responses for interactive applications\n",
    "- **Image generation**: Ability to create and modify images based on text prompts\n",
    "\n",
    "This notebook will guide you through setting up and using the Gemini API with Kotlin, demonstrating various capabilities from basic text generation to more advanced multimodal interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "First, let's set up the dependencies we need. We'll use the `%use` magic command to import the required libraries."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "// load version variables\n",
    "%use @file[resources/version.json](currentDir=\".\")\n",
    "%use lib-ext"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "USE {\n",
    "    repositories {\n",
    "        mavenCentral()\n",
    "    }\n",
    "    dependencies {\n",
    "        implementation(\"com.google.genai:google-genai:$googleGenAiSdkVersion\")\n",
    "        implementation(\"org.jetbrains:markdown-jvm:0.7.3\")\n",
    "    }\n",
    "}\n",
    "// list the library, if the library is not exist, restart kernel\n",
    "notebook.currentClasspath.joinToString(\"\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import required classes for this notebook"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import com.google.genai.Client\n",
    "import com.google.genai.types.GenerateContentConfig\n",
    "import kotlin.jvm.optionals.*\n",
    "import com.google.genai.types.Blob\n",
    "import com.google.genai.types.Content\n",
    "import com.google.genai.types.Part\n",
    "import org.http4k.base64Encode\n",
    "import org.http4k.base64Decoded\n",
    "import org.http4k.base64DecodedArray\n",
    "import org.intellij.markdown.flavours.commonmark.CommonMarkFlavourDescriptor\n",
    "import org.intellij.markdown.html.HtmlGenerator\n",
    "import org.intellij.markdown.parser.MarkdownParser\n",
    "\n",
    "// a bridge to render Markdown text to HTML in the kernal\n",
    "class Markdown(val content: String) : Renderable {\n",
    "    companion object {\n",
    "        private val flavour = CommonMarkFlavourDescriptor()\n",
    "        private val mdParser = MarkdownParser(flavour)\n",
    "    }\n",
    "\n",
    "    override fun render(notebook: Notebook): DisplayResult {\n",
    "        return HTML(HtmlGenerator(content, mdParser.buildMarkdownTreeFromString(content), flavour).generateHtml())\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Project ID and location\n",
    "\n",
    "To use the Google Gemini API, we need to specify our Google Cloud project ID and the location (region) where we want to run the API. The project ID identifies your Google Cloud project, and the location determines where your API requests will be processed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "val projectId = System.getenv(\"GOOGLE_CLOUD_PROJECT\") ?: \"gaplotech\" // Replace with your actual project ID\n",
    "val location = System.getenv(\"GOOGLE_CLOUD_REGION\") ?: \"us-central1\" // This is the default location for Gemini API\n",
    "\n",
    "print(projectId to location)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create GenAI Client\n",
    "\n",
    "Now we'll create a client for the Google Generative AI API. The client is the main entry point for interacting with the Gemini models. We'll configure it to use Vertex AI, which is Google Cloud's managed machine learning platform, and specify our project ID and location.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val client = Client.builder()\n",
    "    .vertexAI(true)\n",
    "    .project(projectId)\n",
    "    .location(location)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Choose model name and prompt\n",
    "\n",
    "Google offers several Gemini models with different capabilities and performance characteristics. Here's a summary of the available models:\n",
    "| Model                                | Inputs                          | Outputs                          | Use Case                                                                 |\n",
    "|--------------------------------------|---------------------------------|----------------------------------|--------------------------------------------------------------------------|\n",
    "| Gemini 2.0 Flash<br>gemini-2.0-flash | Text, Code, Images, Audio, Video, Video with Audio, PDF | Text, Audio (private preview), Images (private preview) | Workhorse model for all daily tasks. Strong overall performance and supports real-time streaming Live API. |\n",
    "| Gemini 2.5 Pro Experimental<br>gemini-2.5-pro-exp-03-25 | Text, Images, Video, Audio, PDF | Text                             | Most advanced reasoning Gemini model, especially for multimodal understanding, coding, and world knowledge. |\n",
    "| Gemini 2.0 Flash-Lite<br>gemini-2.0-flash-lite | Text, Images, Video, Audio, PDF | Text                             | Our cost effective offering to support high throughput.                  |\n",
    "| Gemini 2.0 Flash Thinking<br>gemini-2.0-flash-thinking-exp-01-21 | Text, Images                    | Text                             | Provides stronger reasoning capabilities and includes the thinking process in responses. |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "// Define the model to use\n",
    "val modelName = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "// Simple text prompt example\n",
    "val simplePrompt = \"\"\"\n",
    "Tell me 5 jokes.\n",
    "\"\"\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## API Example 1: Text generation\n",
    "\n",
    "Let's start with a simple text generation example. We'll use the `generateContent` method to send a prompt to the model and get a response. This method is synchronous and returns the complete response at once."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val resp = client.models.generateContent(\n",
    "    modelName,\n",
    "    simplePrompt,\n",
    "    GenerateContentConfig.builder().build()\n",
    ")\n",
    "resp.text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Example 2: Text Generation (Streaming)\n",
    "\n",
    "Let's implement a proper example of using `generateContentStream` with the Vertex API. This method allows us to receive the model's response as a stream of content chunks, which is useful for displaying responses incrementally as they are generated."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val stream = client.models.generateContentStream(\n",
    "    modelName,\n",
    "    simplePrompt,\n",
    "    GenerateContentConfig.builder().build()\n",
    ")\n",
    "\n",
    "stream.forEach { resp ->\n",
    "    print(resp.text())\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use Case 1: Multimodal understanding (understand image input, text output)\n",
    "Here's a brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val imageUrl = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "Image(imageUrl, embed = true).withWidth(300)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's a brain teaser based on an image.\n",
    "\n",
    "> How do I use three of the pool balls to sum up to 30?\n",
    "\n",
    "It cannot be solved mathematically, let's see if the model can do it right."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val resp = client.models.generateContent(\n",
    "    modelName, Content.builder().role(\"user\").parts(\n",
    "        listOf(\n",
    "            Part.builder().inlineData(Blob.builder().data(Image.downloadData(imageUrl)).mimeType(\"image/png\").build()).build(),\n",
    "            Part.builder().text(\"How do I use three of the pool balls to sum up to 30? ONLY using the balls you have!\").build()\n",
    "        )\n",
    "    ).build(), GenerateContentConfig.builder().build()\n",
    ")\n",
    "\n",
    "resp.text()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Use case 2: Create a story book (Text input, Text & Image output)\n",
    "\n",
    "Gemini models can generate both text and images in response to a prompt. In this example, we'll ask the model to create a children's storybook with both narrative text and accompanying images for each episode. We use the `responseModalities` parameter to specify that we want both text and image outputs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val stream = client.models.generateContentStream(\n",
    "    modelName, \"\"\"\n",
    "    Make a children's storybook for age 3-5 about a curious young fox named Mosi, who sets off on a magical adventure through a forest in search of a special star.\n",
    "    The story unfolds over three episodes, with each episode introducing Mosi to a new friend and revealing wondrous and magical landscapes.\n",
    "    For each episode, provide a title, a captivating narrative, and also generate a realistic image illustrating everything in the scene described in the narrative of that episode. Each episode should have less than 50 words.\n",
    "    \"\"\", GenerateContentConfig.builder()\n",
    "        .responseModalities(listOf(\"TEXT\", \"IMAGE\"))\n",
    "        .build()\n",
    ")\n",
    "\n",
    "// initialize a single IPykernel display\n",
    "DISPLAY(\"\", \"story-display\")\n",
    "\n",
    "// accumulate the content from the stream\n",
    "var content = \"\"\n",
    "\n",
    "stream.forEach { resp ->\n",
    "    resp.parts()?.forEach { part ->\n",
    "        if(part.text().isPresent) {\n",
    "            content += part.text().get()\n",
    "        } else if(part.inlineData().isPresent) {\n",
    "            content += Image(part.inlineData().get().data().get(), \"png\").withWidth(400).toHTML()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // update the display\n",
    "    UPDATE_DISPLAY(Markdown(content), \"story-display\")\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Use case 3: Modify image (Text & Image input, Image output)\n",
    "\n",
    "Gemini can also modify or transform images based on text instructions. In this example, we'll provide an input image of pool balls and ask the model to change them to volleyballs. The model will generate a new image based on our request while maintaining the overall composition of the original image."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val imageUrl = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "Image(imageUrl, embed = true).withWidth(300)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val stream = client.models.generateContentStream(\n",
    "    modelName, Content.builder().role(\"user\").parts(\n",
    "        listOf(\n",
    "            Part.builder().inlineData(\n",
    "                Blob.builder().data(Image.downloadData(imageUrl)).mimeType(\"image/png\").build()\n",
    "            ).build(),\n",
    "            Part.builder().text(\"Change the balls to volleyball.\").build()\n",
    "        ),\n",
    "    ).build(), GenerateContentConfig.builder()\n",
    "        .responseModalities(listOf(\"TEXT\", \"IMAGE\"))\n",
    "        .build()\n",
    ")\n",
    "\n",
    "stream.forEach { resp ->\n",
    "    resp.parts()?.forEach { part ->\n",
    "        if (part.text().isPresent) {\n",
    "            DISPLAY(part.text().get())\n",
    "        } else if (part.inlineData().isPresent) {\n",
    "            DISPLAY(Image(part.inlineData().get().data().get(), \"png\").withWidth(400))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored Google's Gemini API using Kotlin. Gemini represents a significant advancement in multimodal AI capabilities, offering powerful features for developers:\n",
    "\n",
    "- **Text generation**: Create high-quality text content for various applications\n",
    "- **Streaming responses**: Get real-time responses for interactive applications\n",
    "- **Multimodal understanding**: Process and reason about text and images together\n",
    "- **Image generation**: Create and modify images based on text instructions\n",
    "\n",
    "The Google GenAI Java SDK provides a convenient way to access these capabilities from Kotlin applications. With just a few lines of code, you can integrate advanced AI features into your applications.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore more advanced prompting techniques\n",
    "- Try different model parameters to control the output\n",
    "- Implement chat-based applications using conversation history\n",
    "- Experiment with more complex multimodal inputs and outputs\n",
    "\n",
    "For more information, check out the [Google Gemini documentation](https://ai.google.dev/docs) and the [Google GenAI Java SDK](https://github.com/google/generative-ai-java)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
