{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running LLM inference with Spring AI & Ollama\n",
    "This notebook implement the basic text-to-text generation using [Spring AI](https://spring.io/projects/spring-ai) and OpenAI.\n",
    "You need to an OpenAI API key to run. Rename `openaikey.example.json` to `openaikey.secret.json` and update the OpenAI key\n",
    "\n",
    "Free feel to contribute to add more use cases."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install dependencies"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "// load version variables\n",
    "%use @file[resources/version.json](currentDir=\".\")"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:55:23.561194Z",
     "start_time": "2025-01-12T13:55:23.027638Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/kotlin-stdlib-1.9.23.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/kotlin-reflect-1.9.23.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/kotlinx-serialization-core-jvm-1.6.3.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/annotations-13.0.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/slf4j-api-2.0.12.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/api-0.12.0-356.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/kotlinx-serialization-json-jvm-1.6.3.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/kotlin-script-runtime-1.9.23.jar\n",
       "/Users/gaplo917/Library/Caches/JetBrains/IntelliJIdea2024.3/kotlinNotebook/kotlin-notebook-standalone.eb20de96/kernels/0.12.0-356/kotlin-jupyter-script-classpath-shadowed-zip_extracted/lib-0.12.0-356.jar"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1,
   "source": [
    "USE {\n",
    "    repositories {\n",
    "        maven { url = \"https://repo.spring.io/milestone\" }\n",
    "        mavenCentral()\n",
    "    }\n",
    "    dependencies {\n",
    "        implementation(\"org.springframework.ai:spring-ai-core:$springAiVersion\")\n",
    "        implementation(\"org.springframework.ai:spring-ai-ollama:$springAiVersion\")\n",
    "        implementation(\"org.springframework.ai:spring-ai-openai:$springAiVersion\")\n",
    "\n",
    "        implementation(\"io.projectreactor:reactor-core:$reactorVersion\")\n",
    "        implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-core-jvm:$kotlinCoroutineVersion\")\n",
    "        implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactor:$kotlinCoroutineVersion\")\n",
    "        implementation(\"org.jetbrains.kotlinx:kotlinx-coroutines-reactive:$kotlinCoroutineVersion\")\n",
    "    }\n",
    "    import(\n",
    "        \"kotlinx.coroutines.*\",\n",
    "        \"kotlinx.coroutines.flow.*\",\n",
    "        \"kotlinx.coroutines.reactor.*\",\n",
    "        \"kotlinx.coroutines.reactive.*\",\n",
    "    )\n",
    "}\n",
    "// list the library, if the dependencies doesn't show up, run again and restart the kernel\n",
    "notebook.currentClasspath.joinToString(\"\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function to measure model performance\n",
    "A sample implementation using stream to measure key metrics\n",
    "- time to first token\n",
    "- input token process rate/s\n",
    "- output token process rate/s"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:59:32.038801Z",
     "start_time": "2025-01-12T13:59:31.907385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.model.ChatModel\n",
    "import org.springframework.ai.chat.prompt.Prompt\n",
    "import kotlin.time.measureTimedValue\n",
    "\n",
    "data class ModelPerformance(\n",
    "    val timeToFirstTokenInMills: Double,\n",
    "    val totalTimeInMills: Double,\n",
    "    val promptTokens: Long,\n",
    "    val generationTokens: Long,\n",
    "    val inputTokenRatePerSec: Double,\n",
    "    val outputTokenRatePerSec: Double\n",
    ")\n",
    "\n",
    "fun ChatModel.measureModelPerformance(prompt: Prompt = Prompt(\"tell me 5 jokes\")): ModelPerformance {\n",
    "    var ttft: Long? = null\n",
    "\n",
    "    val timedResp = measureTimedValue {\n",
    "        runBlocking {\n",
    "            val startTime = System.nanoTime()\n",
    "            stream(prompt)\n",
    "                .asFlow()\n",
    "                .onStart {\n",
    "                    ttft = System.nanoTime() - startTime\n",
    "                }\n",
    "                .onEach {\n",
    "                    print(it.result?.output?.content)\n",
    "                }\n",
    "                .last()\n",
    "        }\n",
    "    }\n",
    "    val resp = timedResp.value\n",
    "    val totalTime = timedResp.duration.inWholeMilliseconds\n",
    "    val timeToFirstTokenInMills = ttft!! / 1_000_000.0\n",
    "    return ModelPerformance(\n",
    "        timeToFirstTokenInMills = timeToFirstTokenInMills,\n",
    "        totalTimeInMills = timedResp.duration.inWholeMilliseconds.toDouble(),\n",
    "        promptTokens = resp.metadata.usage.promptTokens,\n",
    "        generationTokens = resp.metadata.usage.generationTokens,\n",
    "        inputTokenRatePerSec = resp.metadata.usage.promptTokens * 1000.0 / timeToFirstTokenInMills,\n",
    "        outputTokenRatePerSec = resp.metadata.usage.generationTokens * 1000.0 / (totalTime - timeToFirstTokenInMills)\n",
    "    )\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create GPT4o-mini chat model\n",
    "\n",
    "### Load OpenAI Key into Kotlin Notebook\n",
    "Rename `openaikey.example.json` to `openaikey.secret.json` and update the OpenAI key\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:59:29.884851Z",
     "start_time": "2025-01-12T13:59:29.848492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// Load openaikey.json into `openAiKey`\n",
    "%use @file[resources/openaikey.secret.json](currentDir=\".\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:59:35.731832Z",
     "start_time": "2025-01-12T13:59:33.388014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.prompt.Prompt\n",
    "import org.springframework.ai.openai.OpenAiChatModel\n",
    "import org.springframework.ai.openai.OpenAiChatOptions\n",
    "import org.springframework.ai.openai.api.OpenAiApi\n",
    "\n",
    "val model = OpenAiChatModel(\n",
    "    OpenAiApi(openAiKey),\n",
    "    OpenAiChatOptions.builder()\n",
    "        .streamUsage(true)\n",
    "        .model(OpenAiApi.ChatModel.GPT_4_O_MINI)\n",
    "        .temperature(0.7)\n",
    "        .build()\n",
    ")\n",
    "\n",
    "model.measureModelPerformance(Prompt(\"tell me 5 jokes\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are five jokes for you:\n",
      "\n",
      "1. Why did the scarecrow win an award?\n",
      "   Because he was outstanding in his field!\n",
      "\n",
      "2. What do you call fake spaghetti?\n",
      "   An impasta!\n",
      "\n",
      "3. Why donâ€™t scientists trust atoms?\n",
      "   Because they make up everything!\n",
      "\n",
      "4. How do you organize a space party?\n",
      "   You planet!\n",
      "\n",
      "5. Why did the bicycle fall over?\n",
      "   Because it was two-tired!\n",
      "\n",
      "Hope these brought a smile to your face!nullnull"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelPerformance(timeToFirstTokenInMills=17.110708, totalTimeInMills=2107.0, promptTokens=12, generationTokens=99, inputTokenRatePerSec=701.3152231924009, outputTokenRatePerSec=47.37093030667579)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
